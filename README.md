# ğŸ“„ LLM Question & Answer Generator

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io)
[![LangChain](https://img.shields.io/badge/Framework-LangChain-FE1A42.svg)](https://www.langchain.com/)
[![Ollama](https://img.shields.io/badge/Local%20LLM-Ollama%20%7C%20Llama3-000000.svg)](https://ollama.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A powerful **AI-powered application** that generates structured interview questions and detailed answers directly from PDF documents.  
Built on **Retrieval Augmented Generation (RAG)** architecture with a local **Large Language Model (LLM)** for accurate, context-aware Q&A generation.

This project focuses on **automatic question generation using Large Language Models (LLMs)**. The notebook demonstrates how to extract information from text passages and generate meaningful, context-aware questions automatically â€” a process useful for building intelligent tutoring systems, interview preparation tools, and data-driven content curation systems.

## ğŸ“˜ Project Overview

The notebook (`experiment.ipynb`) contains a workflow for:

- Loading and preprocessing text data  
- Using **LLM-based pipelines** to generate contextually relevant questions  
- Refining question quality through prompt engineering  
- Automating evaluation of generated questions  
- Experimenting with different **chain-based architectures** (`LLMChain`, `RefineDocumentsChain`, etc.) for document-level understanding

The code demonstrates how to handle **multi-step reasoning** and **text-to-question transformation** efficiently.

---
## âš™ï¸ How It Works

![flow](flow.png)

1. **PDF Extraction** â€” Extracts and cleans text from the uploaded file.  
2. **Text Chunking** â€” Splits text into manageable overlapping chunks.  
3. **Embedding & Retrieval** â€” Creates vector embeddings and stores them in **FAISS**.  
4. **Question Generation** â€” Uses Llama3 (via Ollama) to produce meaningful questions.  
5. **Answer Generation** â€” Retrieves context and generates grounded answers.  
6. **Export** â€” Saves all pairs to a downloadable CSV file.  
   
---

## âœ¨ Key Features

- ğŸ” **Intelligent Q&A Generation** â€” Generates relevant questions and well-structured answers using **Ollama (Llama3)**.  
- ğŸ§  **RAG Architecture** â€” Uses **FAISS** for efficient document retrieval and grounding.  
- âš¡ **Dual Interfaces** â€” Access via both **FastAPI** (web app) and **Streamlit** (interactive dashboard).  
- ğŸ§© **Flexible Embeddings** â€” Powered by **HuggingFace Sentence Transformers** for semantic search.  
- ğŸ“¤ **Export Results** â€” Download generated Q&A pairs as a clean **CSV file**.  

---


## ğŸ–¥ï¸ Interfaces

<div align="center">

### FastAPI Web Application  
<img src="1.png" alt="FastAPI Interface" style="max-width:80%;border-radius:8px;margin-bottom:20px;">

### Streamlit Dashboard  
<img src="2.png" alt="Streamlit Interface" style="max-width:80%;border-radius:8px;">

</div>

---

## âš™ï¸ Setup and Usage

### Prerequisites

- Python **3.8+**
- [Ollama](https://ollama.ai/) installed and running locally  

### Ollama Setup

```bash
# Install and run Ollama
ollama serve

# Pull the required model
ollama pull llama3
```

### Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running the Applications

### 1ï¸âƒ£ FastAPI Web Application

```bash
python app.py
```

Then open: [http://localhost:8080](http://localhost:8080)

### 2ï¸âƒ£ Streamlit Application

```bash
streamlit run streamlit_app.py
```

Then open: [http://localhost:8501](http://localhost:8501)

---

## ğŸ§© Project Structure

```
interview-questions-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py           # Core pipeline functions
â”‚   â”œâ”€â”€ prompt.py           # LLM prompt templates
â”‚   â””â”€â”€ config.py           # Configuration settings
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # FastAPI web template
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ docs/               # Uploaded PDF files
â”‚   â””â”€â”€ output/             # Generated CSV results
â”œâ”€â”€ app.py                  # FastAPI app entry point
â”œâ”€â”€ streamlit_app.py        # Streamlit dashboard
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ install_dependencies.sh # Setup script
â”œâ”€â”€ start_web_app.sh        # Start FastAPI app
â”œâ”€â”€ start_streamlit.sh      # Start Streamlit app
â””â”€â”€ README.md               # Documentation
```


---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|------------|-------------|
| **LLM** | Ollama (Llama3) |
| **Frameworks** | FastAPI, Streamlit |
| **RAG Engine** | LangChain + FAISS |
| **Embeddings** | Sentence Transformers (`all-MiniLM-L6-v2`) |
| **PDF Processing** | PyPDF / pdfplumber |
| **Storage** | Local file system (CSV export) |

---

## ğŸš€ Future Improvements

- Integration with **evaluation metrics** for automatic scoring  
- Adding support for **multi-lingual question generation**  
- Building a **Streamlit UI** for interactive question generation  
- Incorporating **context memory** to improve continuity between questions  

## ğŸ¤ Contributing

Contributions are welcome!  
To contribute:
1. Fork the repo  
2. Create a feature branch  
3. Make your changes  
4. Test thoroughly  
5. Open a pull request  

---

## ğŸ“œ License

This project is released under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---
